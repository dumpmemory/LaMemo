{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = [\"Hello\", \"there\", \".\", \"I\", \"am\", \"fine\", \".\"]\n",
    "position = [1, 2, 3, 4, 5, 6, 7]\n",
    "sentence = [1, 1, 1, 2, 2, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from mem_transformer import RelPartialLearnableMultiHeadAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def einsum_custom(expression, mats, opt_level=\"O2\"):\n",
    "    if opt_level == \"O1\":\n",
    "        return torch.einsum(expression, tuple(mat.float() for mat in mats))\n",
    "    \n",
    "    return torch.einsum(expression, mats)\n",
    "\n",
    "class LAHMRelPartialLearnableMultiHeadAttn(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n",
    "                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False):\n",
    "        super(LAHMRelPartialLearnableMultiHeadAttn, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.mem_len = mem_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.q_net = nn.Linear(d_model, n_head * d_head, bias=False)\n",
    "        self.kv_net = nn.Linear(d_model, 2 * n_head * d_head, bias=False)\n",
    "        \n",
    "        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.dropatt = nn.Dropout(dropatt)\n",
    "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "    \n",
    "    def _rel_shift(self, x, zero_triu=False): # see Transformer-xl paper appendix B\n",
    "        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n",
    "                               device=x.device, dtype=x.dtype)\n",
    "        x_padded = torch.cat([zero_pad, x], dim=1)\n",
    "\n",
    "        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])\n",
    "\n",
    "        x = x_padded[1:].view_as(x)\n",
    "\n",
    "        if zero_triu:\n",
    "            ones = torch.ones((x.size(0), x.size(1)))\n",
    "            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _rel_shift_future(self, x, zero_triu=False): # LAHM rel shift\n",
    "        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n",
    "                                device=x.device, dtype=x.dtype)\n",
    "        x_padded = torch.cat([x, zero_pad], dim=1)\n",
    "\n",
    "        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])\n",
    "\n",
    "        x = x_padded[:-1].view_as(x)\n",
    "\n",
    "        if zero_triu:\n",
    "            ones = torch.ones((x.size(0), x.size(1)))\n",
    "            x = x * torch.triu(ones, x.size(1) - x.size(0))[:,:,None,None]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, w, r, r_w_bias, r_r_bias, attn_mask=None, mems=None, recur_mems=None, denom=None):\n",
    "        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n",
    "\n",
    "        if mems is not None:\n",
    "            cat = torch.cat([mems, w], 0)\n",
    "            if self.pre_lnorm:\n",
    "                w_head_q = self.q_net(self.layer_norm(cat))\n",
    "                w_head_kv = self.kv_net(self.layer_norm(cat))\n",
    "            else:\n",
    "                w_head_q = self.q_net(cat)\n",
    "                w_head_kv = self.kv_net(cat)\n",
    "            r_head_k = self.r_net(r)\n",
    "\n",
    "            w_head_k, w_head_v = torch.chunk(w_head_kv, 2, dim=-1)\n",
    "            \n",
    "            klen = w_head_k.size(0)\n",
    "            \n",
    "            # Be careful!\n",
    "            if w_head_k.size(0) > qlen:\n",
    "                mem_head_q = w_head_q[:klen - qlen]\n",
    "                mem_head_k = w_head_k[(- 2 * qlen + 1) : (- qlen + 1)]\n",
    "                mem_head_v = w_head_v[(- 2 * qlen + 1) : (- qlen + 1)]\n",
    "\n",
    "            \n",
    "            w_head_q = w_head_q[-qlen:]\n",
    "\n",
    "        else:\n",
    "            if self.pre_lnorm:\n",
    "                w_head_q = self.q_net(self.layer_norm(w))\n",
    "                w_head_kv = self.kv_net(self.layer_norm(w))\n",
    "            else:\n",
    "                w_head_q = self.q_net(w)\n",
    "                w_head_kv = self.kv_net(w)\n",
    "            r_head_k = self.r_net(r)\n",
    "\n",
    "            w_head_k, w_head_v = torch.chunk(w_head_kv, 2, dim=-1)\n",
    "\n",
    "            klen = w_head_k.size(0)\n",
    "\n",
    "        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # klen x bsz x n_head x d_head\n",
    "        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
    "        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
    "\n",
    "        \n",
    "        r_head_k_causal = r_head_k[:klen]\n",
    "        r_head_k_ahead = r_head_k[klen:]\n",
    "\n",
    "        r_head_k_causal = r_head_k_causal.view(klen, self.n_head, self.d_head)                # klen x n_head x d_head\n",
    "\n",
    "        #### compute attention score\n",
    "        rw_head_q = w_head_q + r_w_bias                                         # qlen x bsz x n_head x d_head\n",
    "        AC = einsum_custom('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n",
    "        \n",
    "        \n",
    "        rr_head_q = w_head_q + r_r_bias\n",
    "        BD = einsum_custom('ibnd,jnd->ijbn', (rr_head_q, r_head_k_causal))              # qlen x klen x bsz x n_head\n",
    "        BD = self._rel_shift(BD)\n",
    "\n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_score = AC + BD\n",
    "        attn_score.mul_(self.scale)\n",
    "        \n",
    "\n",
    "\n",
    "        #### compute attention probability\n",
    "        if attn_mask is not None and attn_mask.any().item():\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_score = attn_score.float().masked_fill(\n",
    "                    attn_mask[None,:,:,None].bool(), -float('inf')).type_as(attn_score)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                attn_score = attn_score.float().masked_fill(\n",
    "                    attn_mask[:,:,:,None].bool(), -float('inf')).type_as(attn_score)\n",
    "                \n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_prob = F.softmax(attn_score, dim=1)\n",
    "        attn_prob = self.dropatt(attn_prob)\n",
    "\n",
    "        #### compute attention vector\n",
    "        attn_vec = einsum_custom('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n",
    "\n",
    "        # [qlen x bsz x n_head x d_head]\n",
    "        _attn_vec = attn_vec.contiguous().view(\n",
    "            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n",
    "\n",
    "        ##### linear projection\n",
    "        attn_out = self.o_net(_attn_vec)\n",
    "        attn_out = self.drop(attn_out)\n",
    "\n",
    "        if self.pre_lnorm:\n",
    "            ##### residual connection\n",
    "            output = w + attn_out\n",
    "        else:\n",
    "            ##### residual connection + layer normalization\n",
    "            output = self.layer_norm(w + attn_out)\n",
    "\n",
    "\n",
    "        #### look-ahead memory\n",
    "        if w_head_k.size(0) > qlen:\n",
    "            mem_head_q = mem_head_q.view(klen - qlen, bsz, self.n_head, self.d_head)   # klen - qlen, bsz, n_head, d_head\n",
    "            mem_head_k = mem_head_k.view(qlen, bsz, self.n_head, self.d_head)          # qlen, bsz, n_head, d_head\n",
    "            mem_head_v = mem_head_v.view(qlen, bsz, self.n_head, self.d_head)          # qlen, bsz, n_head, d_head\n",
    "\n",
    "            AC_mem = einsum_custom('ibnd,jbnd->ijbn', (mem_head_q + r_w_bias, mem_head_k))\n",
    "            \n",
    "            r_head_k_ahead = r_head_k_ahead.view(klen - qlen, self.n_head, self.d_head) # klen - qlen, bsz, n_head, d_head\n",
    "            \n",
    "            #print(mem_head_q.size())\n",
    "            #print(r_head_k_ahead.size())\n",
    "            \n",
    "            BD_mem = einsum_custom('ibnd,jnd->ijbn', (mem_head_q + r_r_bias, r_head_k_ahead)) # klen - qlen, klen - qlen, bsz, d_head\n",
    "\n",
    "            assert(BD_mem.size(0) == BD_mem.size(1))\n",
    "            #print(BD_mem.size())\n",
    "            BD_mem = self._rel_shift_future(BD_mem)[:, -qlen:].contiguous() # klen - qlen, qlen, bsz, d_head\n",
    "\n",
    "            attn_score_mem = AC_mem + BD_mem\n",
    "            attn_score_mem.mul_(self.scale)\n",
    "\n",
    "            attn_mask_mem = torch.ones(klen - qlen, klen - qlen, dtype=attn_mask.dtype, device=attn_mask.device)\n",
    "            attn_mask_mem = torch.triu(attn_mask_mem)[:, -qlen:].ne(1).bool()[:, :, None, None]\n",
    "            #print(attn_mask_mem)\n",
    "            attn_score_mem = attn_score_mem.float().masked_fill_(attn_mask_mem, -float('inf')).type_as(attn_score_mem)\n",
    "            #print(attn_score_mem)\n",
    "            \n",
    "            \n",
    "            attn_prob_mem = F.softmax(attn_score_mem, dim=1)\n",
    "            attn_prob_mem = self.dropatt(attn_prob_mem)\n",
    "\n",
    "            attn_vec_mem = einsum_custom('ijbn,jbnd->ibnd', (attn_prob_mem, mem_head_v)) # klen - qlen, bsz, n_head, d_head\n",
    "            \n",
    "            #### memory interpolation\n",
    "            mem_denom = attn_score_mem.exp().sum(dim=1)\n",
    "            \n",
    "            inter_mems = (recur_mems * denom.unsqueeze(-1) + attn_vec_mem * mem_denom.unsqueeze(-1)) / (denom.unsqueeze(-1) + mem_denom.unsqueeze(-1))\n",
    "            \n",
    "            \n",
    "            #### layer norm (non-linear module)\n",
    "            \n",
    "            _inter_mems = inter_mems.contiguous().view(\n",
    "                attn_vec_mem.size(0), attn_vec_mem.size(1), self.n_head * self.d_head)\n",
    "            \n",
    "            attn_out_mem = self.o_net(_inter_mems)\n",
    "            attn_out_mem = self.drop(attn_out_mem)\n",
    "\n",
    "            if self.pre_lnorm:\n",
    "                ##### residual connection\n",
    "                output_mem = mems + attn_out_mem\n",
    "            else:\n",
    "                ##### residual connection + layer normalization\n",
    "                output_mem = self.layer_norm(mems + attn_out_mem)\n",
    "\n",
    "        else:\n",
    "            output_mem = torch.empty(0, dtype=attn_score.dtype, device=attn_score.device)\n",
    "            mem_denom = torch.empty(0, dtype=attn_score.dtype, device=attn_score.device)\n",
    "            inter_mems = torch.empty(0, dtype=attn_score.dtype, device=attn_score.device)\n",
    "        \n",
    "        \n",
    "        #### update denom \n",
    "        cur_denom = attn_score.exp().sum(dim=1) # qlen\n",
    "        #print(attn_score)\n",
    "        \n",
    "        # TODO: should consider mem_len > qlen\n",
    "        prev_denom = (denom + mem_denom) # mlen\n",
    "        new_denom = torch.cat([prev_denom, cur_denom], dim=0) # mlen + qlen\n",
    "        \n",
    "        beg_idx = max(0, klen - self.mem_len)\n",
    "        \n",
    "        # TODO: Are we going to detach it ?\n",
    "        new_denom = new_denom[beg_idx:].detach()\n",
    "        \n",
    "        \n",
    "        #### update recurrent memory\n",
    "        new_recur_mems = torch.cat([inter_mems, attn_vec], dim=0)[beg_idx:].detach()\n",
    "        #print(attn_vec.size())\n",
    "        #print(new_recur_mems.size())\n",
    "\n",
    "        # tgt output\n",
    "        # layer memory output\n",
    "        # recurrent memory output\n",
    "        # recurrent denom\n",
    "        return output, output_mem, new_recur_mems, new_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1\n",
      "LAHM: 0.00693202018737793 s\n",
      "step 2\n",
      "torch.Size([300, 128, 410])\n",
      "torch.Size([150, 128, 410])\n",
      "LAHM: 0.01898646354675293 s\n"
     ]
    }
   ],
   "source": [
    "# first step without memory\n",
    "mem_len = 150\n",
    "device = \"cuda:1\"\n",
    "mlen = 0\n",
    "qlen = 150\n",
    "bsz = 128\n",
    "dim = 410\n",
    "d_head = 41\n",
    "n_head = 10\n",
    "klen = mlen + qlen\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "r_w_bias = nn.Parameter(torch.randn(n_head, dim // n_head)).to(device)\n",
    "r_r_bias = nn.Parameter(torch.randn(n_head, dim // n_head)).to(device)\n",
    "dec_attn_mask = torch.triu(\n",
    "    torch.ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None].to(device)\n",
    "#print(dec_attn_mask)\n",
    "\n",
    "r = torch.arange(klen - 1, - mlen - 1, -1.0).unsqueeze(1).expand(-1, dim).to(device)\n",
    "\n",
    "w = torch.randn(qlen, bsz, dim).to(device)\n",
    "\n",
    "mems = torch.empty(0).to(device)\n",
    "denom = torch.empty(0).to(device)\n",
    "recur_mems = torch.empty(0).to(device)\n",
    "\n",
    "net = LAHMRelPartialLearnableMultiHeadAttn(n_head, dim, d_head, 0.1, tgt_len=qlen, mem_len=mem_len).to(device)\n",
    "\n",
    "\n",
    "start_time_lahm = time.time()\n",
    "net.eval()\n",
    "hidden, next_layer_mems, recur_mems, recur_denom = net(w, r, r_w_bias, r_r_bias, dec_attn_mask, mems, recur_mems, denom)\n",
    "\n",
    "print(\"step 1\")\n",
    "print(\"LAHM: {} s\".format(time.time() - start_time_lahm))\n",
    "\n",
    "print(\"step 2\")\n",
    "\n",
    "\n",
    "mlen = qlen\n",
    "klen = 2 * qlen\n",
    "r = torch.arange(klen - 1, - qlen - 1, -1.0).unsqueeze(1).expand(-1, dim).to(device)\n",
    "dec_attn_mask = torch.triu(\n",
    "    torch.ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None].to(device)\n",
    "mems = torch.randn(qlen, bsz, dim).to(device)\n",
    "\n",
    "start_time_lahm_2 = time.time()\n",
    "net.eval()\n",
    "hidden, next_layer_mems, recur_mems, recur_denom = net(w, r, r_w_bias, r_r_bias, dec_attn_mask, mems, recur_mems, recur_denom)\n",
    "#print(hidden.size())\n",
    "print(\"LAHM: {} s\".format(time.time() - start_time_lahm_2))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1\n",
      "xl: 0.0069615840911865234 s\n",
      "step 2\n",
      "xl: 0.017897844314575195 s\n"
     ]
    }
   ],
   "source": [
    "mem_len = 150\n",
    "device = \"cuda:1\"\n",
    "mlen = 0\n",
    "qlen = 150\n",
    "bsz = 128\n",
    "dim = 410\n",
    "d_head = 41\n",
    "n_head = 10\n",
    "klen = mlen + qlen\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "r_w_bias = nn.Parameter(torch.randn(n_head, dim // n_head)).to(device)\n",
    "r_r_bias = nn.Parameter(torch.randn(n_head, dim // n_head)).to(device)\n",
    "dec_attn_mask = torch.triu(\n",
    "    torch.ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None].to(device)\n",
    "#print(dec_attn_mask)\n",
    "\n",
    "r = torch.arange(klen - 1, - qlen - 1, -1.0).unsqueeze(1).expand(-1, dim).to(device)\n",
    "\n",
    "w = torch.randn(qlen, bsz, dim).to(device)\n",
    "\n",
    "mems = torch.empty(0).to(device)\n",
    "denom = torch.empty(0).to(device)\n",
    "recur_mems = torch.empty(0).to(device)\n",
    "\n",
    "net2 = RelPartialLearnableMultiHeadAttn(n_head, dim, d_head, 0.1, tgt_len=qlen, mem_len=mem_len).to(device)\n",
    "\n",
    "\n",
    "\n",
    "start_time_xl = time.time()\n",
    "net2.eval()\n",
    "hid = net2(w, r[:klen], r_w_bias, r_r_bias, dec_attn_mask, torch.empty(0).to(device))\n",
    "print(\"step 1\")\n",
    "print(\"xl: {} s\".format(time.time() - start_time_xl))\n",
    "\n",
    "mlen = qlen\n",
    "klen = 2 * qlen\n",
    "r = torch.arange(klen - 1, - qlen - 1, -1.0).unsqueeze(1).expand(-1, dim).to(device)\n",
    "dec_attn_mask = torch.triu(\n",
    "    torch.ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None].to(device)\n",
    "mems = torch.randn(qlen, bsz, dim).to(device)\n",
    "\n",
    "start_time_xl_2 = time.time()\n",
    "net2.eval()\n",
    "hid = net2(w, r[:klen], r_w_bias, r_r_bias, dec_attn_mask, mems)\n",
    "print(\"step 2\")\n",
    "print(\"xl: {} s\".format(time.time() - start_time_xl_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.],\n",
      "        [ 4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.],\n",
      "        [ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.],\n",
      "        [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "        [-2., -2., -2., -2., -2., -2., -2., -2.]])\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 4])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recur_mems.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1894, -0.6018, -0.3084,  0.2983],\n",
       "        [ 0.0254,  1.0685, -0.2839,  0.2252],\n",
       "        [-0.9691,  0.1838, -0.9397, -0.2222]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,4)\n",
    "b = torch.empty(0)\n",
    "torch.cat([a,b],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recur_denom.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
